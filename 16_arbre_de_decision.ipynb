{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ae916100",
   "metadata": {},
   "source": [
    "## Cours sur les Arbres de Décision en Machine Learning\n",
    "\n",
    "### Introduction\n",
    "\n",
    "Les **arbres de décision** (ou *decision trees* en anglais) sont des algorithmes de machine learning utilisés pour résoudre des problèmes de classification et de régression. Leur principe repose sur une structure arborescente où chaque nœud interne représente une condition sur une caractéristique (ou attribut), chaque branche représente le résultat de cette condition, et chaque feuille représente une décision ou une prédiction.\n",
    "\n",
    "### 1. Structure d'un Arbre de Décision\n",
    "\n",
    "Un arbre de décision est composé de plusieurs éléments :\n",
    "\n",
    "- **Racine (Root Node)** : Le nœud racine est le premier nœud de l'arbre. Il contient l'ensemble des données d'entraînement et correspond à la caractéristique qui sépare le mieux les données.\n",
    "\n",
    "- **Nœuds Internes (Internal Nodes)** : Chaque nœud interne représente un test ou une condition sur une caractéristique. Le test divise les données en sous-groupes.\n",
    "\n",
    "- **Branches** : Les branches relient les nœuds et représentent les résultats des tests effectués sur les caractéristiques.\n",
    "\n",
    "- **Feuilles (Leaf Nodes)** : Les feuilles sont les nœuds terminaux de l'arbre, qui fournissent la classification ou la prédiction finale.\n",
    "\n",
    "### 2. Construction d'un Arbre de Décision\n",
    "\n",
    "La construction d'un arbre de décision repose sur le concept de *partitionnement récursif* des données :\n",
    "\n",
    "1. **Choix de la meilleure caractéristique** : Pour chaque nœud, l'arbre choisit la caractéristique qui permet de mieux séparer les données en fonction de l'objectif (maximiser l'homogénéité des sous-groupes).\n",
    "\n",
    "2. **Partition des données** : Les données sont divisées en sous-ensembles en fonction de la caractéristique choisie.\n",
    "\n",
    "3. **Récursion** : Ce processus est répété récursivement pour chaque sous-ensemble jusqu'à ce que certaines conditions d'arrêt soient atteintes (par exemple, toutes les données appartiennent à la même classe, ou un seuil de profondeur maximale est atteint).\n",
    "\n",
    "### 3. Mesures de Séparation\n",
    "\n",
    "Pour choisir la meilleure caractéristique à chaque nœud, différentes mesures sont utilisées :\n",
    "\n",
    "- **L'Entropie et le Gain d'Information** : L'entropie mesure le désordre ou l'incertitude dans un ensemble de données. Le gain d'information évalue la réduction de l'entropie obtenue en divisant les données selon une certaine caractéristique.\n",
    "\n",
    "    \\[\n",
    "    \\text{Gain d'information} = \\text{Entropie}(D) - \\sum_{v \\in V} \\frac{|D_v|}{|D|} \\times \\text{Entropie}(D_v)\n",
    "    \\]\n",
    "    Où \\(D\\) est l'ensemble des données et \\(D_v\\) est le sous-ensemble des données pour la valeur \\(v\\) de la caractéristique.\n",
    "\n",
    "- **L'Indice de Gini** : L'indice de Gini est une mesure de l'impureté d'une distribution. Il est utilisé pour évaluer la qualité d'une division. Un indice de Gini de 0 correspond à une pureté maximale.\n",
    "\n",
    "    \\[\n",
    "    \\text{Gini}(D) = 1 - \\sum_{i=1}^{n} p_i^2\n",
    "    \\]\n",
    "    Où \\(p_i\\) est la proportion d'éléments appartenant à la classe \\(i\\) dans \\(D\\).\n",
    "\n",
    "- **Le Gain de Gini** : Il mesure la réduction de l'indice de Gini après un split, similaire au gain d'information.\n",
    "\n",
    "### 4. Avantages et Inconvénients des Arbres de Décision\n",
    "\n",
    "**Avantages** :\n",
    "\n",
    "- **Interprétabilité** : Les arbres de décision sont faciles à visualiser et à interpréter, ce qui les rend populaires pour les applications nécessitant une explication claire des décisions.\n",
    "\n",
    "- **Prise en compte des interactions** : Ils capturent les interactions entre les caractéristiques sans nécessiter de transformations complexes des données.\n",
    "\n",
    "- **Peu de prétraitement** : Ils ne nécessitent pas de normalisation ou de mise à l'échelle des caractéristiques.\n",
    "\n",
    "**Inconvénients** :\n",
    "\n",
    "- **Sur-apprentissage (overfitting)** : Les arbres de décision peuvent facilement sur-apprendre les données d'entraînement, en particulier s'ils sont trop profonds.\n",
    "\n",
    "- **Instabilité** : Un petit changement dans les données peut entraîner un arbre de décision complètement différent.\n",
    "\n",
    "- **Biais** : Ils peuvent être biaisés envers les caractéristiques ayant plus de niveaux de valeurs.\n",
    "\n",
    "### 5. Techniques pour Améliorer les Arbres de Décision\n",
    "\n",
    "Pour surmonter les inconvénients des arbres de décision, plusieurs techniques peuvent être utilisées :\n",
    "\n",
    "- **Élagage (Pruning)** : L'élagage consiste à couper les branches peu informatives de l'arbre pour réduire le sur-apprentissage.\n",
    "\n",
    "- **Arbres de Décision Ensemblistes** : Combiner plusieurs arbres de décision pour améliorer les performances, par exemple avec des techniques comme les forêts aléatoires (*random forests*) ou le boosting (ex. : *Gradient Boosting*).\n",
    "\n",
    "- **Régularisation** : Imposer des pénalités pour des arbres trop complexes afin de contrôler la profondeur ou le nombre de feuilles.\n",
    "\n",
    "### 6. Arbres de Décision en Python avec Scikit-learn\n",
    "\n",
    "Pour illustrer l'utilisation des arbres de décision, utilisons Python et la bibliothèque scikit-learn.\n",
    "\n",
    "```python\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn import metrics\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.tree import plot_tree\n",
    "\n",
    "# Chargement du dataset Iris\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "# Séparation des données en ensembles d'entraînement et de test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=1)\n",
    "\n",
    "# Création du classificateur d'arbre de décision\n",
    "clf = DecisionTreeClassifier()\n",
    "\n",
    "# Entraînement du modèle\n",
    "clf = clf.fit(X_train, y_train)\n",
    "\n",
    "# Prédiction sur l'ensemble de test\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "# Evaluation du modèle\n",
    "print(\"Accuracy:\", metrics.accuracy_score(y_test, y_pred))\n",
    "\n",
    "# Visualisation de l'arbre de décision\n",
    "plt.figure(figsize=(12,8))\n",
    "plot_tree(clf, filled=True)\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "### 7. Applications des Arbres de Décision\n",
    "\n",
    "Les arbres de décision sont utilisés dans divers domaines tels que :\n",
    "\n",
    "- **Classification d'images** : Identifier les objets dans une image.\n",
    "- **Analyse du risque de crédit** : Prédire si un client remboursera un prêt.\n",
    "- **Diagnostic médical** : Aider à diagnostiquer des maladies en fonction des symptômes.\n",
    "\n",
    "### Conclusion\n",
    "\n",
    "Les arbres de décision sont des outils puissants et faciles à comprendre pour la classification et la régression. Ils constituent une excellente introduction au machine learning tout en étant suffisamment flexibles pour être utilisés dans des solutions industrielles complexes. Toutefois, il est essentiel de les combiner avec d'autres techniques pour améliorer leur performance et réduire le risque de sur-apprentissage.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16e088cf",
   "metadata": {},
   "source": [
    "### Forêts Aléatoires (Random Forests)\n",
    "\n",
    "Les **forêts aléatoires** sont une technique d'ensembliste très populaire en machine learning. Elles combinent plusieurs arbres de décision pour améliorer la précision et la robustesse du modèle, réduisant ainsi les risques de sur-apprentissage (*overfitting*).\n",
    "\n",
    "#### 1. Principe des Forêts Aléatoires\n",
    "\n",
    "L'idée principale derrière les forêts aléatoires est de créer plusieurs arbres de décision indépendants à partir de sous-échantillons du jeu de données original et de combiner leurs prédictions.\n",
    "\n",
    "**Étapes clés** :\n",
    "\n",
    "- **Bagging (Bootstrap Aggregating)** : Chaque arbre est construit à partir d'un sous-échantillon aléatoire du jeu de données avec remplacement (bootstrap). Cela signifie que certains échantillons peuvent apparaître plusieurs fois dans le sous-échantillon, tandis que d'autres peuvent être absents.\n",
    "  \n",
    "- **Randomisation des Caractéristiques** : Lors de la création de chaque nœud dans un arbre, un sous-ensemble aléatoire de caractéristiques est sélectionné. Cela introduit encore plus de diversité parmi les arbres.\n",
    "\n",
    "- **Agrégation des Prédictions** : Pour la classification, la prédiction finale est obtenue par vote majoritaire parmi tous les arbres. Pour la régression, c'est la moyenne des prédictions des arbres.\n",
    "\n",
    "#### 2. Avantages des Forêts Aléatoires\n",
    "\n",
    "- **Réduction du Sur-apprentissage** : En moyenne, les forêts aléatoires ont tendance à sur-apprendre moins que les arbres de décision individuels car les erreurs des arbres individuels sont en moyenne plus faibles.\n",
    "\n",
    "- **Robustesse** : En combinant plusieurs arbres, les forêts aléatoires sont moins sensibles aux variations dans les données d'entraînement et donc plus stables.\n",
    "\n",
    "- **Gestion des Données Manquantes** : Les forêts aléatoires peuvent mieux gérer les données manquantes que les arbres de décision individuels.\n",
    "\n",
    "#### 3. Implémentation en Python\n",
    "\n",
    "Voici un exemple de mise en œuvre avec scikit-learn :\n",
    "\n",
    "```python\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import metrics\n",
    "\n",
    "# Chargement du dataset Iris\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "# Séparation des données en ensembles d'entraînement et de test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=1)\n",
    "\n",
    "# Création du classificateur de forêt aléatoire\n",
    "clf = RandomForestClassifier(n_estimators=100, random_state=1)\n",
    "\n",
    "# Entraînement du modèle\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Prédiction sur l'ensemble de test\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "# Evaluation du modèle\n",
    "print(\"Accuracy:\", metrics.accuracy_score(y_test, y_pred))\n",
    "```\n",
    "\n",
    "### Boosting (Exemple : Gradient Boosting)\n",
    "\n",
    "Le **boosting** est une autre technique d'ensembliste, mais contrairement au bagging utilisé dans les forêts aléatoires, le boosting construit les modèles séquentiellement. Chaque modèle essaie de corriger les erreurs des modèles précédents.\n",
    "\n",
    "#### 1. Principe du Boosting\n",
    "\n",
    "Dans le boosting, les modèles sont construits de manière itérative, en attribuant plus de poids aux échantillons mal prédits par les modèles précédents. Le Gradient Boosting est une méthode de boosting qui optimise un critère de perte de manière itérative en ajustant les erreurs résiduelles.\n",
    "\n",
    "**Étapes clés** :\n",
    "\n",
    "- **Initialisation** : Commencez par un modèle simple (par exemple, un seul arbre de décision).\n",
    "\n",
    "- **Apprentissage Séquentiel** : Chaque nouvel arbre est entraîné sur les erreurs résiduelles (les différences entre les prédictions du modèle actuel et les valeurs réelles).\n",
    "\n",
    "- **Mise à Jour du Modèle** : Les prédictions sont mises à jour en ajoutant les prédictions pondérées du nouvel arbre au modèle existant.\n",
    "\n",
    "#### 2. Gradient Boosting\n",
    "\n",
    "Le **Gradient Boosting** est une technique de boosting spécifique où l'optimisation est effectuée par une descente de gradient sur l'erreur résiduelle.\n",
    "\n",
    "**Avantages** :\n",
    "\n",
    "- **Précision élevée** : Les modèles de boosting comme le Gradient Boosting peuvent obtenir une très haute précision sur des jeux de données complexes.\n",
    "\n",
    "- **Flexibilité** : Peut être utilisé pour la classification, la régression, et même pour des tâches plus complexes comme le classement.\n",
    "\n",
    "**Inconvénients** :\n",
    "\n",
    "- **Sur-apprentissage** : Plus susceptible de sur-apprendre que les forêts aléatoires, surtout si le nombre d'itérations est élevé.\n",
    "\n",
    "- **Temps d'entraînement** : Peut être plus lent à entraîner, surtout avec un grand nombre d'itérations.\n",
    "\n",
    "#### 3. Implémentation en Python\n",
    "\n",
    "Voici un exemple de mise en œuvre de Gradient Boosting avec scikit-learn :\n",
    "\n",
    "```python\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn import metrics\n",
    "\n",
    "# Chargement du dataset Iris\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "# Séparation des données en ensembles d'entraînement et de test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=1)\n",
    "\n",
    "# Création du classificateur de Gradient Boosting\n",
    "clf = GradientBoostingClassifier(n_estimators=100, learning_rate=0.1, random_state=1)\n",
    "\n",
    "# Entraînement du modèle\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Prédiction sur l'ensemble de test\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "# Evaluation du modèle\n",
    "print(\"Accuracy:\", metrics.accuracy_score(y_test, y_pred))\n",
    "```\n",
    "\n",
    "### Conclusion\n",
    "\n",
    "Les **forêts aléatoires** et le **boosting** sont deux des techniques d'ensembliste les plus puissantes en machine learning. Les forêts aléatoires sont robustes et faciles à utiliser, tandis que le boosting, bien que potentiellement plus complexe, offre une précision exceptionnelle. Comprendre ces deux méthodes et leurs différences est essentiel pour appliquer efficacement les algorithmes d'ensembliste à des problèmes complexes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9843fd7a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
