{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0ade38b1",
   "metadata": {},
   "source": [
    "### **Cours : Apprentissage Supervisé - Régression**\n",
    "\n",
    "#### **1. Introduction à l'Apprentissage Supervisé**\n",
    "   - **Définition de l'Apprentissage Supervisé**\n",
    "     - Apprentissage supervisé : L'algorithme apprend à partir de données étiquetées, c'est-à-dire que chaque exemple d'entraînement est accompagné de la réponse correcte.\n",
    "   - **Qu'est-ce que la Régression?**\n",
    "     - La régression est une technique de machine learning supervisé utilisée pour prédire une valeur continue. Contrairement à la classification, où les sorties sont des catégories, la régression vise à prédire des valeurs numériques.\n",
    "   - **Exemples d'Applications de la Régression**\n",
    "     - Prédiction du prix d'une maison en fonction de ses caractéristiques (superficie, nombre de chambres, etc.).\n",
    "     - Estimation de la consommation d'énergie en fonction de la température.\n",
    "     - Prévision des ventes futures en fonction des tendances historiques.\n",
    "     "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4662b362",
   "metadata": {},
   "source": [
    "#### **2. Modèles de Régression en Apprentissage Supervisé**\n",
    "   - **Régression Linéaire Simple**\n",
    "     - **Concept** : Modèle qui cherche à établir une relation linéaire entre une variable indépendante (X) et une variable dépendante (Y).\n",
    "     - **Équation** : \\( Y = aX + b \\) où \\(a\\) est la pente et \\(b\\) est l'ordonnée à l'origine.\n",
    "     - **Exemple** : Prédire le prix d'une maison en fonction de sa superficie.\n",
    "     - **Avantages** : Simple à comprendre et à interpréter.\n",
    "     - **Inconvénients** : Ne fonctionne pas bien si la relation entre les variables n'est pas linéaire."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feb06681",
   "metadata": {},
   "source": [
    "<img src=\"image/lineaire.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dab1246",
   "metadata": {},
   "source": [
    "### **1. Contexte : Régression Linéaire**\n",
    "Pour une régression linéaire simple, le modèle prédit \\( \\hat{y} = w_1 x + w_0 \\), où :\n",
    "- \\( w_1 \\) est le coefficient (ou pente),\n",
    "- \\( w_0 \\) est l'ordonnée à l'origine (ou biais).\n",
    "\n",
    "L'objectif est de trouver les valeurs optimales de \\( w_1 \\) et \\( w_0 \\) qui minimisent l'erreur quadratique moyenne (MSE) entre les prédictions \\( \\hat{y} \\) et les valeurs réelles \\( y \\).\n",
    "\n",
    "### **2. Fonction Coût (MSE)**\n",
    "La fonction coût \\( J(w_0, w_1) \\) pour la régression linéaire est donnée par la MSE :\n",
    "\n",
    "\\[\n",
    "J(w_0, w_1) = \\frac{1}{n} \\sum_{i=1}^{n} \\left( y_i - (w_1 x_i + w_0) \\right)^2\n",
    "\\]\n",
    "\n",
    "### **3. Calcul des Dérivées Partielles**\n",
    "Pour minimiser \\( J(w_0, w_1) \\), on utilise la méthode de la descente de gradient, qui nécessite de calculer les dérivées partielles de \\( J \\) par rapport à \\( w_0 \\) et \\( w_1 \\).\n",
    "\n",
    "Les dérivées partielles sont :\n",
    "- Par rapport à \\( w_1 \\) (pente) :\n",
    "  \n",
    "\\[\n",
    "\\frac{\\partial J}{\\partial w_1} = -\\frac{2}{n} \\sum_{i=1}^{n} x_i \\left( y_i - (w_1 x_i + w_0) \\right)\n",
    "\\]\n",
    "\n",
    "- Par rapport à \\( w_0 \\) (biais) :\n",
    "\n",
    "\\[\n",
    "\\frac{\\partial J}{\\partial w_0} = -\\frac{2}{n} \\sum_{i=1}^{n} \\left( y_i - (w_1 x_i + w_0) \\right)\n",
    "\\]\n",
    "\n",
    "### **4. Mise à Jour des Coefficients : Descente de Gradient**\n",
    "Une fois les dérivées partielles calculées, les coefficients \\( w_1 \\) et \\( w_0 \\) sont mis à jour en fonction de la direction du gradient pour minimiser l'erreur :\n",
    "\n",
    "\\[\n",
    "w_1 := w_1 - \\alpha \\frac{\\partial J}{\\partial w_1}\n",
    "\\]\n",
    "\n",
    "\\[\n",
    "w_0 := w_0 - \\alpha \\frac{\\partial J}{\\partial w_0}\n",
    "\\]\n",
    "\n",
    "Où :\n",
    "- \\( \\alpha \\) est le taux d'apprentissage, un petit nombre positif qui contrôle la taille des pas vers le minimum de la fonction coût.\n",
    "\n",
    "### **5. Boucle d'Optimisation**\n",
    "Ce processus est répété pour plusieurs itérations (ou jusqu'à ce que la convergence soit atteinte), en réévaluant les dérivées et en ajustant les coefficients à chaque étape.\n",
    "\n",
    "### **Exemple de Code : Descente de Gradient pour Régression Linéaire**\n",
    "Voici un exemple de code Python illustrant la descente de gradient manuelle pour une régression linéaire simple :\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "\n",
    "# Données d'exemple\n",
    "X = np.array([1, 2, 3, 4, 5])\n",
    "Y = np.array([2, 4, 6, 8, 10])\n",
    "\n",
    "# Paramètres initiaux\n",
    "w1 = 0\n",
    "w0 = 0\n",
    "alpha = 0.01  # Taux d'apprentissage\n",
    "iterations = 1000  # Nombre d'itérations\n",
    "\n",
    "n = len(Y)  # Nombre de points de données\n",
    "\n",
    "# Boucle de descente de gradient\n",
    "for i in range(iterations):\n",
    "    Y_pred = w1 * X + w0  # Prédiction du modèle\n",
    "    error = Y - Y_pred\n",
    "    \n",
    "    # Calcul des dérivées partielles\n",
    "    dw1 = -2/n * np.sum(X * error)\n",
    "    dw0 = -2/n * np.sum(error)\n",
    "    \n",
    "    # Mise à jour des paramètres\n",
    "    w1 = w1 - alpha * dw1\n",
    "    w0 = w0 - alpha * dw0\n",
    "    \n",
    "    # Optionnel : Afficher le coût à chaque étape\n",
    "    cost = np.mean(error ** 2)\n",
    "    if i % 100 == 0:\n",
    "        print(f'Iteration {i}: w1 = {w1}, w0 = {w0}, Cost = {cost}')\n",
    "\n",
    "print(f'Modèle final: w1 = {w1}, w0 = {w0}')\n",
    "```\n",
    "\n",
    "### **Résumé :**\n",
    "- La dérivée de la fonction coût par rapport aux paramètres du modèle (pente et biais) est calculée pour déterminer la direction dans laquelle ajuster les paramètres.\n",
    "- La descente de gradient est utilisée pour itérer vers les valeurs optimales des paramètres, minimisant ainsi l'erreur quadratique moyenne.\n",
    "\n",
    "Cette approche est fondamentale pour l'entraînement des modèles de machine learning, où l'objectif est d'ajuster les paramètres de manière à minimiser l'erreur entre les prédictions du modèle et les valeurs réelles."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f53672a9",
   "metadata": {},
   "source": [
    "$$\n",
    "\\text{MSE} = \\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2\n",
    "$$\n",
    "\n",
    "L'**Erreur Quadratique Moyenne** (Mean Squared Error ou MSE) est une métrique utilisée pour évaluer la performance d'un modèle de régression. Elle mesure la moyenne des carrés des erreurs entre les valeurs prédites par le modèle et les valeurs réelles observées dans les données.\n",
    "\n",
    "### **Définition et Formule :**\n",
    "\n",
    "L'erreur quadratique moyenne est calculée en prenant la moyenne des carrés des différences entre chaque prédiction et la valeur réelle correspondante. Mathématiquement, la MSE est définie comme suit :\n",
    "\n",
    "\\[\n",
    "\\text{MSE} = \\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2\n",
    "\\]\n",
    "\n",
    "- \\( n \\) : le nombre total d'observations.\n",
    "- \\( y_i \\) : la valeur réelle de l'observation \\( i \\).\n",
    "- \\( \\hat{y}_i \\) : la valeur prédite par le modèle pour l'observation \\( i \\).\n",
    "- \\( (y_i - \\hat{y}_i)^2 \\) : l'erreur quadratique pour l'observation \\( i \\).\n",
    "\n",
    "### **Interprétation :**\n",
    "- **MSE = 0** : Cela signifie que le modèle prédit parfaitement toutes les valeurs (aucune différence entre les valeurs réelles et les valeurs prédites).\n",
    "- **MSE élevée** : Cela indique que les prédictions du modèle sont loin des valeurs réelles, ce qui signifie que le modèle n'est pas très performant.\n",
    "- **Unité** : L'unité de la MSE est le carré de l'unité de la variable à prédire. Par exemple, si vous prédisez un prix en dollars, la MSE sera en dollars au carré.\n",
    "\n",
    "### **Pourquoi utiliser le MSE ?**\n",
    "1. **Sensibilité aux grandes erreurs** : Le MSE punit davantage les grandes erreurs en les quadrillant, ce qui signifie que de grosses erreurs auront un impact plus important sur la MSE qu'une simple erreur absolue moyenne (MAE). Cela peut être utile si vous voulez que votre modèle évite de grandes erreurs.\n",
    "   \n",
    "2. **Convexité** : La MSE est une fonction convexe par rapport aux paramètres du modèle. Cela signifie qu'il est plus facile d'optimiser un modèle en minimisant la MSE, car les algorithmes d'optimisation peuvent efficacement trouver le minimum global de cette fonction.\n",
    "\n",
    "### **Exemple :**\n",
    "Supposons que vous ayez les valeurs réelles suivantes pour un jeu de données :\n",
    "\n",
    "\\[\n",
    "Y = [2, 3, 5, 7]\n",
    "\\]\n",
    "\n",
    "Et que votre modèle prédise les valeurs suivantes :\n",
    "\n",
    "\\[\n",
    "\\hat{Y} = [2.5, 3, 4.5, 6.5]\n",
    "\\]\n",
    "\n",
    "L'erreur pour chaque point est calculée comme suit :\n",
    "\n",
    "\\[\n",
    "(2 - 2.5)^2 = 0.25, \\quad (3 - 3)^2 = 0, \\quad (5 - 4.5)^2 = 0.25, \\quad (7 - 6.5)^2 = 0.25\n",
    "\\]\n",
    "\n",
    "La MSE serait :\n",
    "\n",
    "\\[\n",
    "\\text{MSE} = \\frac{0.25 + 0 + 0.25 + 0.25}{4} = \\frac{0.75}{4} = 0.1875\n",
    "\\]\n",
    "\n",
    "### **Limitation :**\n",
    "La MSE étant une moyenne des erreurs au carré, elle peut être sensible aux valeurs aberrantes (outliers), car les erreurs sont quadrillées. Une seule prédiction très éloignée de la réalité peut fortement influencer la MSE.\n",
    "\n",
    "C'est une mesure largement utilisée en régression, mais il est souvent utile de la combiner avec d'autres métriques (comme le \\(R^2\\) ou la MAE) pour avoir une évaluation complète de la performance du modèle."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a369e695",
   "metadata": {},
   "source": [
    "- **Régression Linéaire Multiple**\n",
    "     - **Concept** : Extension de la régression linéaire simple pour inclure plusieurs variables indépendantes.\n",
    "     - **Équation** : \\( Y = a_1X_1 + a_2X_2 + ... + a_nX_n + b \\) où \\( X_1, X_2, ..., X_n \\) sont les variables indépendantes.\n",
    "     - **Exemple** : Prédire le prix d'une maison en fonction de la superficie, du nombre de chambres, de l'âge de la maison, etc.\n",
    "     - **Avantages** : Peut modéliser la relation entre plusieurs variables et la variable cible.\n",
    "     - **Inconvénients** : Plus complexe à interpréter, sensible à la multicolinéarité."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe162a0d",
   "metadata": {},
   "source": [
    "- **Régression Polynomiale**\n",
    "     - **Concept** : Étend la régression linéaire pour modéliser des relations non linéaires en ajoutant des termes de puissance de la variable indépendante.\n",
    "     - **Équation** : \\( Y = aX^2 + bX + c \\) (pour un polynôme de degré 2).\n",
    "     - **Exemple** : Prédire la croissance des revenus en fonction du temps où la relation n'est pas linéaire.\n",
    "     - **Avantages** : Peut mieux s'ajuster aux données qui montrent une tendance non linéaire.\n",
    "     - **Inconvénients** : Risque de surajustement (overfitting) si le degré du polynôme est trop élevé."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5357d1f8",
   "metadata": {},
   "source": [
    "#### **3. Métriques d'Évaluation des Modèles de Régression**\n",
    "   - **Erreur Quadratique Moyenne (Mean Squared Error - MSE)**\n",
    "     - Mesure la moyenne des carrés des erreurs, c'est-à-dire la différence entre les valeurs prédites et les valeurs réelles.\n",
    "   - **Erreur Absolue Moyenne (Mean Absolute Error - MAE)**\n",
    "     - Moyenne des valeurs absolues des erreurs.\n",
    "   - **Coefficient de Détermination (\\(R^2\\))**\n",
    "     - Indique la proportion de la variance dans la variable dépendante qui est prévisible à partir des variables indépendantes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb958ee9",
   "metadata": {},
   "source": [
    "#### **4. Cas Pratiques**\n",
    "   - **Prédiction du Prix d'une Maison avec Régression Linéaire Multiple**\n",
    "     - Utilisation des caractéristiques comme la superficie, l'emplacement, et l'âge de la maison pour prédire son prix.\n",
    "   - **Régression Polynomiale pour Modéliser la Relation entre l'Expérience et le Salaire**\n",
    "     - Utilisation d'une régression polynomiale pour capturer une relation non linéaire entre le nombre d'années d'expérience et le salaire.\n",
    "   - **Ridge et Lasso pour la Sélection de Caractéristiques dans une Grande Base de Données**\n",
    "     - Comparaison de la performance de ces modèles dans un contexte où il y a de nombreuses variables explicatives."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55bf4e5d",
   "metadata": {},
   "source": [
    "\n",
    "#### **5. Conclusion**\n",
    "   - **Résumé des Concepts Clés**\n",
    "     - Comprendre le type de relation entre les variables est crucial pour choisir le bon modèle de régression.\n",
    "   - **Ressources Supplémentaires**\n",
    "     - Tutoriels en ligne, documentation sur les bibliothèques Python (Scikit-learn), livres sur le machine learning."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
