{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e0c22ddb",
   "metadata": {},
   "source": [
    "### Cours sur l'algorithme K-Nearest Neighbors (KNN)\n",
    "\n",
    "#### Introduction\n",
    "\n",
    "L'algorithme **K-Nearest Neighbors (KNN)** est un algorithme d'apprentissage supervisé utilisé à la fois pour la classification et la régression. Il est basé sur le principe que des points similaires dans l'espace des caractéristiques sont proches les uns des autres. En d'autres termes, les objets similaires sont proches dans l'espace des données.\n",
    "\n",
    "#### Principe de fonctionnement\n",
    "\n",
    "Le KNN fonctionne en trouvant les **K voisins les plus proches** d'un point donné (à prédire) et en prenant une décision en fonction des classes ou des valeurs cibles de ces voisins.\n",
    "\n",
    "##### Classification\n",
    "\n",
    "- **Etape 1 :** Lorsqu'un nouvel échantillon doit être classé, l'algorithme trouve les K points les plus proches (les plus similaires) dans l'ensemble de données d'entraînement.\n",
    "- **Etape 2 :** L'algorithme effectue un vote majoritaire parmi les classes de ces K voisins pour déterminer la classe du nouvel échantillon.\n",
    "- **Etape 3 :** Le nouvel échantillon est attribué à la classe la plus fréquente parmi ses K voisins.\n",
    "\n",
    "##### Régression\n",
    "\n",
    "- **Etape 1 :** Lorsqu'une nouvelle valeur doit être prédite, l'algorithme trouve les K points les plus proches dans l'ensemble de données d'entraînement.\n",
    "- **Etape 2 :** La valeur prédite est calculée en faisant la moyenne des valeurs cibles de ces K voisins.\n",
    "\n",
    "#### Choix de K\n",
    "\n",
    "Le paramètre **K** est crucial pour les performances du KNN. Un petit K rend l'algorithme sensible au bruit (surapprentissage), tandis qu'un K trop grand peut rendre la classification trop simple (sous-apprentissage). Il est souvent conseillé d'utiliser une technique de validation croisée pour choisir la meilleure valeur de K.\n",
    "\n",
    "#### Mesure de distance\n",
    "\n",
    "La distance entre les points peut être mesurée de différentes manières. Les plus courantes sont :\n",
    "\n",
    "- **Distance Euclidienne :** \\( d(p, q) = \\sqrt{\\sum_{i=1}^{n} (p_i - q_i)^2} \\)\n",
    "- **Distance de Manhattan :** \\( d(p, q) = \\sum_{i=1}^{n} |p_i - q_i| \\)\n",
    "- **Distance de Minkowski :** Une généralisation de la distance euclidienne et de Manhattan.\n",
    "\n",
    "Le choix de la mesure de distance dépend du problème et de la nature des données.\n",
    "\n",
    "#### Avantages et Inconvénients\n",
    "\n",
    "**Avantages :**\n",
    "\n",
    "1. **Simplicité :** L'algorithme est facile à comprendre et à implémenter.\n",
    "2. **Pas de phase d'apprentissage :** Il est basé sur la recherche de voisins lors de la prédiction, ce qui signifie qu'il n'y a pas de phase d'entraînement.\n",
    "3. **Flexible :** Il peut être utilisé pour la classification et la régression.\n",
    "\n",
    "**Inconvénients :**\n",
    "\n",
    "1. **Complexité computationnelle :** Le KNN peut être lent lors de la prédiction, surtout avec des ensembles de données de grande taille.\n",
    "2. **Sensibilité aux échelles des données :** Les caractéristiques ayant des amplitudes différentes peuvent fausser les résultats, d'où l'importance de normaliser les données.\n",
    "3. **Sensible au bruit :** Si K est trop petit, l'algorithme peut être influencé par les anomalies ou le bruit dans les données.\n",
    "\n",
    "#### Exemple d'implémentation en Python avec `sklearn`\n",
    "\n",
    "Voici un exemple simple d'utilisation du KNN pour une tâche de classification :\n",
    "\n",
    "```python\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Charger les données Iris\n",
    "data = load_iris()\n",
    "X = data.data\n",
    "y = data.target\n",
    "\n",
    "# Séparer les données en ensembles d'entraînement et de test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Normaliser les données\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Créer et entraîner le modèle KNN\n",
    "knn = KNeighborsClassifier(n_neighbors=3)\n",
    "knn.fit(X_train, y_train)\n",
    "\n",
    "# Prédire les classes des données de test\n",
    "y_pred = knn.predict(X_test)\n",
    "\n",
    "# Calculer l'exactitude\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Exactitude : {accuracy * 100:.2f}%\")\n",
    "```\n",
    "\n",
    "#### Conclusion\n",
    "\n",
    "L'algorithme KNN est un outil puissant pour les tâches de classification et de régression, notamment en raison de sa simplicité. Cependant, il doit être utilisé avec précaution, notamment en ce qui concerne le choix du paramètre K et la normalisation des données."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eb3bc1b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
