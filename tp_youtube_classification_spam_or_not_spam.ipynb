{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "387792fb",
   "metadata": {},
   "source": [
    "Le **`TfidfVectorizer`** est un outil de la bibliothèque `scikit-learn` qui transforme du texte brut en une représentation numérique appelée **matrice TF-IDF** (Term Frequency - Inverse Document Frequency). Cette méthode est couramment utilisée pour convertir des données textuelles en vecteurs utilisables dans des algorithmes de machine learning.\n",
    "\n",
    "### Détails sur `TfidfVectorizer` :\n",
    "\n",
    "1. **Term Frequency (TF)** :\n",
    "   - **TF** mesure la fréquence d'apparition d'un mot dans un document par rapport au nombre total de mots dans ce document.\n",
    "   - La formule est :\n",
    "       TF(t,d)= Nombre total de termes dans le document d / Nombre de fois que le terme t apparat dans le document d\n",
    " \n",
    "   - Cela donne un score de fréquence pour chaque mot dans chaque document.\n",
    "\n",
    "2. **Inverse Document Frequency (IDF)** :\n",
    "   - **IDF** mesure l'importance d'un mot en fonction de sa rareté dans l'ensemble des documents.\n",
    "   - Si un mot est très fréquent dans plusieurs documents, son importance est réduite, car il n'apporte pas d'information discriminante.\n",
    "   - La formule est :\n",
    "     \\[\n",
    "     \\text{IDF}(t, D) = \\log\\left(\\frac{\\text{Nombre total de documents } |D|}{\\text{Nombre de documents contenant le terme } t} + 1\\right)\n",
    "     \\]\n",
    "   - Cela donne un poids plus faible aux mots communs (comme \"le\", \"la\", etc.) et plus élevé aux mots rares.\n",
    "\n",
    "3. **TF-IDF Score** :\n",
    "   - Le score TF-IDF pour chaque terme est le produit de TF et IDF :\n",
    "       IDF(t,D)=log( Nombre de documents contenant le terme t / Nombre total de documents ∣D∣ +1)\n",
    "     \n",
    "   - Le résultat est une matrice où chaque document est représenté par un vecteur de scores TF-IDF pour chaque mot.\n",
    "\n",
    "### Avantages du TF-IDF :\n",
    "- **Réduit l'impact des mots fréquents** : Les mots très fréquents mais peu significatifs (comme \"de\", \"le\", \"et\") obtiennent des scores faibles.\n",
    "- **Mots rares et importants** : Les mots plus rares, mais discriminants (qui apparaissent dans peu de documents), ont des scores plus élevés, ce qui permet d'améliorer la pertinence dans les tâches de classification ou de recherche.\n",
    "\n",
    "### Comment utiliser `TfidfVectorizer` :\n",
    "\n",
    "Voici quelques paramètres importants de `TfidfVectorizer` et comment l'utiliser :\n",
    "\n",
    "```python\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Créer un exemple de corpus\n",
    "corpus = [\n",
    "    \"I love programming in Python.\",\n",
    "    \"Python is a great programming language.\",\n",
    "    \"I love data science and machine learning.\"\n",
    "]\n",
    "\n",
    "# Initialiser le TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer(\n",
    "    stop_words='english',   # Exclut les mots fréquents et peu informatifs (comme \"and\", \"the\")\n",
    "    max_features=1000,      # Limite à 1000 les termes les plus fréquents\n",
    "    ngram_range=(1, 2)      # Prend en compte les unigrammes et bigrammes (mots seuls et paires de mots)\n",
    ")\n",
    "\n",
    "# Transformer le texte en vecteurs TF-IDF\n",
    "X_tfidf = vectorizer.fit_transform(corpus)\n",
    "\n",
    "# Afficher les noms des termes et la matrice TF-IDF\n",
    "print(\"Features (termes):\", vectorizer.get_feature_names_out())\n",
    "print(\"Matrice TF-IDF:\")\n",
    "print(X_tfidf.toarray())\n",
    "```\n",
    "\n",
    "### Paramètres utiles de `TfidfVectorizer` :\n",
    "- **`stop_words`** : Permet de supprimer des mots courants (comme \"the\", \"is\", \"and\") qui ne sont pas significatifs pour la classification.\n",
    "- **`max_features`** : Définit le nombre maximal de termes à conserver, en prenant les termes les plus fréquents.\n",
    "- **`ngram_range`** : Définit la taille des n-grammes à prendre en compte. Par exemple, `(1, 1)` signifie seulement des mots individuels (unigrammes), et `(1, 2)` signifie des unigrammes et des bigrammes (paires de mots).\n",
    "- **`min_df`** : Définit le seuil minimal de documents dans lesquels un terme doit apparaître pour être pris en compte.\n",
    "- **`max_df`** : Définit le seuil maximal pour ignorer les termes trop fréquents dans tous les documents (par exemple, ignorer les termes présents dans plus de 90 % des documents).\n",
    "\n",
    "### Exemple de sortie :\n",
    "\n",
    "Si l'on prend le corpus suivant :\n",
    "```python\n",
    "corpus = [\"I love programming in Python.\",\n",
    "          \"Python is a great programming language.\",\n",
    "          \"I love data science and machine learning.\"]\n",
    "```\n",
    "\n",
    "L'utilisation de `TfidfVectorizer` sur ce corpus peut générer une matrice TF-IDF où chaque ligne représente un document, et chaque colonne représente un mot (ou n-gramme) avec ses valeurs de TF-IDF associées.\n",
    "\n",
    "### Conclusion :\n",
    "`TfidfVectorizer` est un outil efficace pour transformer du texte en vecteurs numériques, tout en réduisant l'importance des mots fréquents peu discriminants et en donnant plus de poids aux termes rares mais importants pour la tâche de classification ou d'extraction d'informations.\n",
    "\n",
    "IDF(t,D)=log( \n",
    "Nombre de documents contenant le terme t\n",
    "Nombre total de documents ∣D∣\n",
    "​\n",
    " +1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cde17e24",
   "metadata": {},
   "source": [
    "Prenons un exemple concret de corpus avec quelques phrases, puis calculons manuellement les scores TF-IDF pour illustrer le processus.\n",
    "\n",
    "### Exemple de corpus :\n",
    "Supposons que nous avons trois documents dans notre corpus :\n",
    "\n",
    "```text\n",
    "1. \"Le chat mange une souris.\"\n",
    "2. \"Le chien mange une pomme.\"\n",
    "3. \"Le chat et le chien jouent ensemble.\"\n",
    "```\n",
    "\n",
    "#### Étape 1 : Calculer la fréquence des termes (TF)\n",
    "La **TF (Term Frequency)** est le nombre de fois qu'un mot apparaît dans un document, divisé par le nombre total de mots dans ce document.\n",
    "\n",
    "| Terme   | Document 1           | Document 2           | Document 3                |\n",
    "|---------|----------------------|----------------------|---------------------------|\n",
    "| le      | 1/5 = 0.2            | 1/5 = 0.2            | 1/7 ≈ 0.14                |\n",
    "| chat    | 1/5 = 0.2            | 0                    | 1/7 ≈ 0.14                |\n",
    "| mange   | 1/5 = 0.2            | 1/5 = 0.2            | 0                         |\n",
    "| une     | 1/5 = 0.2            | 1/5 = 0.2            | 0                         |\n",
    "| souris  | 1/5 = 0.2            | 0                    | 0                         |\n",
    "| chien   | 0                    | 1/5 = 0.2            | 1/7 ≈ 0.14                |\n",
    "| pomme   | 0                    | 1/5 = 0.2            | 0                         |\n",
    "| et      | 0                    | 0                    | 1/7 ≈ 0.14                |\n",
    "| jouent  | 0                    | 0                    | 1/7 ≈ 0.14                |\n",
    "| ensemble| 0                    | 0                    | 1/7 ≈ 0.14                |\n",
    "\n",
    "#### Étape 2 : Calculer l'IDF (Inverse Document Frequency)\n",
    "L'**IDF** évalue l'importance d'un mot en fonction de sa rareté dans l'ensemble des documents. Moins un mot apparaît dans de nombreux documents, plus il sera important.\n",
    "\n",
    "- La formule pour l'IDF est :\n",
    "    IDF(t,D)=log( nt/N +1)\n",
    "\n",
    "\n",
    "Où :\n",
    "- \\(N\\) est le nombre total de documents.\n",
    "- \\(n_t\\) est le nombre de documents contenant le terme \\(t\\).\n",
    "\n",
    "Calculons l'IDF pour chaque terme :\n",
    "\n",
    "| Terme    | Nombre de documents contenant le terme \\(n_t\\) | IDF (avec \\(N=3\\))                         |\n",
    "|----------|-----------------------------------------------|--------------------------------------------|\n",
    "| le       | 3 documents                                    | \\(\\log\\left(\\frac{3}{3} + 1\\right) = 0\\)   |\n",
    "| chat     | 2 documents                                    | \\(\\log\\left(\\frac{3}{2} + 1\\right) ≈ 0.41\\)|\n",
    "| mange    | 2 documents                                    | \\(\\log\\left(\\frac{3}{2} + 1\\right) ≈ 0.41\\)|\n",
    "| une      | 2 documents                                    | \\(\\log\\left(\\frac{3}{2} + 1\\right) ≈ 0.41\\)|\n",
    "| souris   | 1 document                                     | \\(\\log\\left(\\frac{3}{1} + 1\\right) ≈ 0.69\\)|\n",
    "| chien    | 2 documents                                    | \\(\\log\\left(\\frac{3}{2} + 1\\right) ≈ 0.41\\)|\n",
    "| pomme    | 1 document                                     | \\(\\log\\left(\\frac{3}{1} + 1\\right) ≈ 0.69\\)|\n",
    "| et       | 1 document                                     | \\(\\log\\left(\\frac{3}{1} + 1\\right) ≈ 0.69\\)|\n",
    "| jouent   | 1 document                                     | \\(\\log\\left(\\frac{3}{1} + 1\\right) ≈ 0.69\\)|\n",
    "| ensemble | 1 document                                     | \\(\\log\\left(\\frac{3}{1} + 1\\right) ≈ 0.69\\)|\n",
    "\n",
    "#### Étape 3 : Calculer les scores TF-IDF\n",
    "Pour chaque terme, multiplions la **TF** par l'**IDF**.\n",
    "\n",
    "Exemple pour le terme **\"chat\"** dans le document 1 :\n",
    "- **TF (chat, doc1)** = 0.2\n",
    "- **IDF (chat)** ≈ 0.41\n",
    "\n",
    "    - TF-IDF(chat, doc1)=0.2×0.41≈0.082\n",
    "\n",
    "\n",
    "Faisons le calcul pour chaque terme :\n",
    "\n",
    "| Terme    | Document 1           | Document 2           | Document 3                |\n",
    "|----------|----------------------|----------------------|---------------------------|\n",
    "| le       | 0                    | 0                    | 0                         |\n",
    "| chat     | 0.082                | 0                    | 0.058                     |\n",
    "| mange    | 0.082                | 0.082                | 0                         |\n",
    "| une      | 0.082                | 0.082                | 0                         |\n",
    "| souris   | 0.138                | 0                    | 0                         |\n",
    "| chien    | 0                    | 0.082                | 0.058                     |\n",
    "| pomme    | 0                    | 0.138                | 0                         |\n",
    "| et       | 0                    | 0                    | 0.098                     |\n",
    "| jouent   | 0                    | 0                    | 0.098                     |\n",
    "| ensemble | 0                    | 0                    | 0.098                     |\n",
    "\n",
    "### Conclusion :\n",
    "Les valeurs **TF-IDF** obtenues montrent à quel point chaque terme est important dans un document en particulier. Par exemple, dans le document 1, le mot **\"souris\"** a une valeur TF-IDF plus élevée que **\"chat\"**, car il n'apparaît que dans ce document et donc a un IDF plus élevé.\n",
    "\n",
    "Le **`TfidfVectorizer`** fait ces calculs automatiquement et permet d'obtenir une matrice TF-IDF pour une analyse plus approfondie."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "40d51e20",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Import des librairies\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, ConfusionMatrixDisplay\n",
    "from sklearn.neighbors import KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "af06bc26",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Lire le corpus\n",
    "corpus = pd.read_csv('datasets/classification/Youtube-Spam-Dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5467d447",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>COMMENT_ID</th>\n",
       "      <th>AUTHOR</th>\n",
       "      <th>DATE</th>\n",
       "      <th>CONTENT</th>\n",
       "      <th>VIDEO_NAME</th>\n",
       "      <th>CLASS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LZQPQhLyRh80UYxNuaDWhIGQYNQ96IuCg-AYWqNPjpU</td>\n",
       "      <td>Julius NM</td>\n",
       "      <td>2013-11-07T06:20:48</td>\n",
       "      <td>Huh, anyway check out this you[tube] channel: ...</td>\n",
       "      <td>PSY - GANGNAM STYLE(?????) M/V</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>LZQPQhLyRh_C2cTtd9MvFRJedxydaVW-2sNg5Diuo4A</td>\n",
       "      <td>adam riyati</td>\n",
       "      <td>2013-11-07T12:37:15</td>\n",
       "      <td>Hey guys check out my new channel and our firs...</td>\n",
       "      <td>PSY - GANGNAM STYLE(?????) M/V</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>LZQPQhLyRh9MSZYnf8djyk0gEF9BHDPYrrK-qCczIY8</td>\n",
       "      <td>Evgeny Murashkin</td>\n",
       "      <td>2013-11-08T17:34:21</td>\n",
       "      <td>just for test I have to say murdev.com</td>\n",
       "      <td>PSY - GANGNAM STYLE(?????) M/V</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>z13jhp0bxqncu512g22wvzkasxmvvzjaz04</td>\n",
       "      <td>ElNino Melendez</td>\n",
       "      <td>2013-11-09T08:28:43</td>\n",
       "      <td>me shaking my sexy ass on my channel enjoy ^_^ ﻿</td>\n",
       "      <td>PSY - GANGNAM STYLE(?????) M/V</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>z13fwbwp1oujthgqj04chlngpvzmtt3r3dw</td>\n",
       "      <td>GsMega</td>\n",
       "      <td>2013-11-10T16:05:38</td>\n",
       "      <td>watch?v=vtaRGgvGtWQ   Check this out .﻿</td>\n",
       "      <td>PSY - GANGNAM STYLE(?????) M/V</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1951</th>\n",
       "      <td>_2viQ_Qnc6-bMSjqyL1NKj57ROicCSJV5SwTrw-RFFA</td>\n",
       "      <td>Katie Mettam</td>\n",
       "      <td>2013-07-13T13:27:39.441000</td>\n",
       "      <td>I love this song because we sing it at Camp al...</td>\n",
       "      <td>Shakira - Waka Waka</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1952</th>\n",
       "      <td>_2viQ_Qnc6-pY-1yR6K2FhmC5i48-WuNx5CumlHLDAI</td>\n",
       "      <td>Sabina Pearson-Smith</td>\n",
       "      <td>2013-07-13T13:14:30.021000</td>\n",
       "      <td>I love this song for two reasons: 1.it is abou...</td>\n",
       "      <td>Shakira - Waka Waka</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1953</th>\n",
       "      <td>_2viQ_Qnc6_k_n_Bse9zVhJP8tJReZpo8uM2uZfnzDs</td>\n",
       "      <td>jeffrey jules</td>\n",
       "      <td>2013-07-13T12:09:31.188000</td>\n",
       "      <td>wow</td>\n",
       "      <td>Shakira - Waka Waka</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1954</th>\n",
       "      <td>_2viQ_Qnc6_yBt8UGMWyg3vh0PulTqcqyQtdE7d4Fl0</td>\n",
       "      <td>Aishlin Maciel</td>\n",
       "      <td>2013-07-13T11:17:52.308000</td>\n",
       "      <td>Shakira u are so wiredo</td>\n",
       "      <td>Shakira - Waka Waka</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1955</th>\n",
       "      <td>_2viQ_Qnc685RPw1aSa1tfrIuHXRvAQ2rPT9R06KTqA</td>\n",
       "      <td>Latin Bosch</td>\n",
       "      <td>2013-07-12T22:33:27.916000</td>\n",
       "      <td>Shakira is the best dancer</td>\n",
       "      <td>Shakira - Waka Waka</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1956 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       COMMENT_ID                AUTHOR  \\\n",
       "0     LZQPQhLyRh80UYxNuaDWhIGQYNQ96IuCg-AYWqNPjpU             Julius NM   \n",
       "1     LZQPQhLyRh_C2cTtd9MvFRJedxydaVW-2sNg5Diuo4A           adam riyati   \n",
       "2     LZQPQhLyRh9MSZYnf8djyk0gEF9BHDPYrrK-qCczIY8      Evgeny Murashkin   \n",
       "3             z13jhp0bxqncu512g22wvzkasxmvvzjaz04       ElNino Melendez   \n",
       "4             z13fwbwp1oujthgqj04chlngpvzmtt3r3dw                GsMega   \n",
       "...                                           ...                   ...   \n",
       "1951  _2viQ_Qnc6-bMSjqyL1NKj57ROicCSJV5SwTrw-RFFA          Katie Mettam   \n",
       "1952  _2viQ_Qnc6-pY-1yR6K2FhmC5i48-WuNx5CumlHLDAI  Sabina Pearson-Smith   \n",
       "1953  _2viQ_Qnc6_k_n_Bse9zVhJP8tJReZpo8uM2uZfnzDs         jeffrey jules   \n",
       "1954  _2viQ_Qnc6_yBt8UGMWyg3vh0PulTqcqyQtdE7d4Fl0        Aishlin Maciel   \n",
       "1955  _2viQ_Qnc685RPw1aSa1tfrIuHXRvAQ2rPT9R06KTqA           Latin Bosch   \n",
       "\n",
       "                            DATE  \\\n",
       "0            2013-11-07T06:20:48   \n",
       "1            2013-11-07T12:37:15   \n",
       "2            2013-11-08T17:34:21   \n",
       "3            2013-11-09T08:28:43   \n",
       "4            2013-11-10T16:05:38   \n",
       "...                          ...   \n",
       "1951  2013-07-13T13:27:39.441000   \n",
       "1952  2013-07-13T13:14:30.021000   \n",
       "1953  2013-07-13T12:09:31.188000   \n",
       "1954  2013-07-13T11:17:52.308000   \n",
       "1955  2013-07-12T22:33:27.916000   \n",
       "\n",
       "                                                CONTENT  \\\n",
       "0     Huh, anyway check out this you[tube] channel: ...   \n",
       "1     Hey guys check out my new channel and our firs...   \n",
       "2                just for test I have to say murdev.com   \n",
       "3      me shaking my sexy ass on my channel enjoy ^_^ ﻿   \n",
       "4               watch?v=vtaRGgvGtWQ   Check this out .﻿   \n",
       "...                                                 ...   \n",
       "1951  I love this song because we sing it at Camp al...   \n",
       "1952  I love this song for two reasons: 1.it is abou...   \n",
       "1953                                                wow   \n",
       "1954                            Shakira u are so wiredo   \n",
       "1955                         Shakira is the best dancer   \n",
       "\n",
       "                          VIDEO_NAME  CLASS  \n",
       "0     PSY - GANGNAM STYLE(?????) M/V      1  \n",
       "1     PSY - GANGNAM STYLE(?????) M/V      1  \n",
       "2     PSY - GANGNAM STYLE(?????) M/V      1  \n",
       "3     PSY - GANGNAM STYLE(?????) M/V      1  \n",
       "4     PSY - GANGNAM STYLE(?????) M/V      1  \n",
       "...                              ...    ...  \n",
       "1951            Shakira - Waka Waka       0  \n",
       "1952            Shakira - Waka Waka       0  \n",
       "1953            Shakira - Waka Waka       0  \n",
       "1954            Shakira - Waka Waka       0  \n",
       "1955            Shakira - Waka Waka       0  \n",
       "\n",
       "[1956 rows x 6 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e669215e",
   "metadata": {},
   "source": [
    "- 0 : non spam\n",
    "- 1 : spam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "72ab47e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extraire le content\n",
    "content = corpus['CONTENT']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "59c22a3b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       Huh, anyway check out this you[tube] channel: ...\n",
       "1       Hey guys check out my new channel and our firs...\n",
       "2                  just for test I have to say murdev.com\n",
       "3        me shaking my sexy ass on my channel enjoy ^_^ ﻿\n",
       "4                 watch?v=vtaRGgvGtWQ   Check this out .﻿\n",
       "                              ...                        \n",
       "1951    I love this song because we sing it at Camp al...\n",
       "1952    I love this song for two reasons: 1.it is abou...\n",
       "1953                                                  wow\n",
       "1954                              Shakira u are so wiredo\n",
       "1955                           Shakira is the best dancer\n",
       "Name: CONTENT, Length: 1956, dtype: object"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "38420b37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conversion du content en vecteur numérique\n",
    "vectorizer = TfidfVectorizer(\n",
    "    stop_words='english',   # Exclut les mots fréquents et peu informatifs (comme \"and\", \"the\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cc4e8fbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformer le texte en vecteurs TF-IDF\n",
    "X_tfidf = vectorizer.fit_transform(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1b985bac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 2232)\t0.5882545194693168\n",
      "  (0, 906)\t0.25136004712214494\n",
      "  (0, 3844)\t0.5199041842695195\n",
      "  (0, 922)\t0.18052433165792894\n",
      "  (0, 1977)\t0.5365494869004906\n",
      "  (1, 3600)\t0.1615250713032538\n",
      "  (1, 1008)\t0.2191571400249832\n",
      "  (1, 2313)\t0.1587212657894938\n",
      "  (1, 2289)\t0.28890275999870085\n",
      "  (1, 3366)\t0.35787697875792346\n",
      "  (1, 4073)\t0.337703313012582\n",
      "  (1, 2592)\t0.3721904223921933\n",
      "  (1, 2593)\t0.39236408813753476\n",
      "  (1, 3952)\t0.337703313012582\n",
      "  (1, 2680)\t0.2133820409144466\n",
      "  (1, 1830)\t0.19577240006612226\n",
      "  (1, 1912)\t0.20522790201939378\n",
      "  (1, 906)\t0.16765643512991726\n",
      "  (1, 922)\t0.12040921477577482\n",
      "  (2, 997)\t0.23710314736860486\n",
      "  (2, 2635)\t0.5793680944794691\n",
      "  (2, 3281)\t0.44773158254114803\n",
      "  (2, 3723)\t0.5793680944794691\n",
      "  (2, 2179)\t0.26829787494789914\n",
      "  (3, 1415)\t0.42798470252107945\n",
      "  :\t:\n",
      "  (1947, 2680)\t0.5025515213504276\n",
      "  (1947, 922)\t0.2835844751079011\n",
      "  (1948, 3252)\t0.5617776241774657\n",
      "  (1948, 4009)\t0.8272882816609469\n",
      "  (1949, 3482)\t0.7969142088945247\n",
      "  (1949, 679)\t0.6040924959491005\n",
      "  (1950, 3349)\t1.0\n",
      "  (1951, 852)\t0.6625634266280975\n",
      "  (1951, 3405)\t0.5273425431138745\n",
      "  (1951, 2393)\t0.2799582624649586\n",
      "  (1951, 3766)\t0.37680346571164935\n",
      "  (1951, 3475)\t0.2501241040410068\n",
      "  (1952, 3119)\t0.4041818645050407\n",
      "  (1952, 472)\t0.6541869082733915\n",
      "  (1952, 773)\t0.3686560236040392\n",
      "  (1952, 3498)\t0.383400579771369\n",
      "  (1952, 2393)\t0.17078221942092323\n",
      "  (1952, 679)\t0.2707863285020225\n",
      "  (1952, 3475)\t0.15258256442472298\n",
      "  (1953, 4137)\t1.0\n",
      "  (1954, 4096)\t0.858626651105238\n",
      "  (1954, 3349)\t0.5126014767944043\n",
      "  (1955, 1159)\t0.7784291666329214\n",
      "  (1955, 3349)\t0.4647234509693391\n",
      "  (1955, 702)\t0.42199543439985676\n"
     ]
    }
   ],
   "source": [
    "print(X_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "49c3d40b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1956, 4229)\n"
     ]
    }
   ],
   "source": [
    "print(X_tfidf.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f6d07f2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = corpus['CLASS']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e7684c2e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1956,)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8b4e3808",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_tfidf,y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2526da8",
   "metadata": {},
   "source": [
    "### Entrainement avec la regression logistique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5121d50",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
